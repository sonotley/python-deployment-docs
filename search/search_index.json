{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This site is currently a work in progress. Most pages are written but I'm still tweaking it. I am posting updates most weeks. What is this all about? Python is pretty easy to write, but when it comes to packaging, distributing and running code somewhere other than your own computer, things get a bit tricky. This is a guide to how I've chosen to tackle that problem. It's a reference for my future self and hopefully of help to other people too. Who is this for? In general, I have two target use-cases when I'm writing code, if you have similar needs perhaps you'll find this site useful. I'm providing a utility for users to use on their workstations - generally Windows PCs I'm proving software to be run as a service on a server, often Windows but sometimes Linux These two use-cases share some similarities: I can't guarantee the version of Python (or even the existence of Python) on the target system I don't want the installer or user to need to know any Python or to do lots of setup work, it needs to just install and work I don't want the deployed code to be scattered around non-obvious places, ideally everything should be in one directory so it can be uninstalled simply by deleting that directory In the case of deployment to a server there is an important addition to the third point above: The installed software must not be dependent on the continued existence of the user account by which it was installed. Who is this not for? Well, I like to think anyone with an interest in Python could get something from these pages, but there are a few area I'm not planning on addressing simply because they aren't options I generally use. They aren't bad options, they just aren't part of the environment I'm currently working in. They are: Deploying software as a Docker container Deploying to 'serverless' cloud services such as Google App Engine, AWS Lambda, Heroku I also don't really cover the online CI/CD tools such as those provided Github and Gitlab. I'd like to explore these, but they aren't in my current workflow.","title":"Introduction"},{"location":"#introduction","text":"This site is currently a work in progress. Most pages are written but I'm still tweaking it. I am posting updates most weeks.","title":"Introduction"},{"location":"#what-is-this-all-about","text":"Python is pretty easy to write, but when it comes to packaging, distributing and running code somewhere other than your own computer, things get a bit tricky. This is a guide to how I've chosen to tackle that problem. It's a reference for my future self and hopefully of help to other people too.","title":"What is this all about?"},{"location":"#who-is-this-for","text":"In general, I have two target use-cases when I'm writing code, if you have similar needs perhaps you'll find this site useful. I'm providing a utility for users to use on their workstations - generally Windows PCs I'm proving software to be run as a service on a server, often Windows but sometimes Linux These two use-cases share some similarities: I can't guarantee the version of Python (or even the existence of Python) on the target system I don't want the installer or user to need to know any Python or to do lots of setup work, it needs to just install and work I don't want the deployed code to be scattered around non-obvious places, ideally everything should be in one directory so it can be uninstalled simply by deleting that directory In the case of deployment to a server there is an important addition to the third point above: The installed software must not be dependent on the continued existence of the user account by which it was installed.","title":"Who is this for?"},{"location":"#who-is-this-not-for","text":"Well, I like to think anyone with an interest in Python could get something from these pages, but there are a few area I'm not planning on addressing simply because they aren't options I generally use. They aren't bad options, they just aren't part of the environment I'm currently working in. They are: Deploying software as a Docker container Deploying to 'serverless' cloud services such as Google App Engine, AWS Lambda, Heroku I also don't really cover the online CI/CD tools such as those provided Github and Gitlab. I'd like to explore these, but they aren't in my current workflow.","title":"Who is this not for?"},{"location":"dev/","text":"Development environment The basics My development environment consists of: Pycharm with Poetry plugin Poetry Git Pycharm is largely just a personal preference, you can follow this workflow with any IDE, or none. Similarly Git is central to how I share, backup and manage the history of my source code, but has no place in my deployment workflow other than as a way to get the source to the place it will be packaged. Poetry however, is pretty important. What is Poetry? I've never heard of it. Poetry is a fairly young project which provides an all-in-one solution to packaging and dependency management. I use Poetry because it makes it very easy to package my code up into something that someone else can pip install (or indeed poetry install but as you'll see, I don't want to rely on the target system having Poetry installed). The workflow When starting a new project, either create it using poetry new or select Poetry from the available interpreters in Pycharm. This will create a basic project structure including two packages, one for your source and one for your tests . It will also create a virtual environment (not in the project directory by default) and pyproject.toml file. The pyproject.toml seems to be the source of some mild controversy in the Python community, but we can ignore that. The great thing for us is that it's the only configuration file we need for our project. To install additional packages from PyPI, rather than using pip , just use poetry add . This will install the package into your environment and add it to the dependencies section of pyproject.toml . You can add dev dependencies with the --dev switch. Talking of dev dependencies... As well as the test dependencies I find the following useful to have in my dev environment. Black for code formatting Pydocstyle to check my docstrings mypy to check my type hints (Pycharm does this automatically)","title":"Development environment"},{"location":"dev/#development-environment","text":"","title":"Development environment"},{"location":"dev/#the-basics","text":"My development environment consists of: Pycharm with Poetry plugin Poetry Git Pycharm is largely just a personal preference, you can follow this workflow with any IDE, or none. Similarly Git is central to how I share, backup and manage the history of my source code, but has no place in my deployment workflow other than as a way to get the source to the place it will be packaged. Poetry however, is pretty important.","title":"The basics"},{"location":"dev/#what-is-poetry-ive-never-heard-of-it","text":"Poetry is a fairly young project which provides an all-in-one solution to packaging and dependency management. I use Poetry because it makes it very easy to package my code up into something that someone else can pip install (or indeed poetry install but as you'll see, I don't want to rely on the target system having Poetry installed).","title":"What is Poetry? I've never heard of it."},{"location":"dev/#the-workflow","text":"When starting a new project, either create it using poetry new or select Poetry from the available interpreters in Pycharm. This will create a basic project structure including two packages, one for your source and one for your tests . It will also create a virtual environment (not in the project directory by default) and pyproject.toml file. The pyproject.toml seems to be the source of some mild controversy in the Python community, but we can ignore that. The great thing for us is that it's the only configuration file we need for our project. To install additional packages from PyPI, rather than using pip , just use poetry add . This will install the package into your environment and add it to the dependencies section of pyproject.toml . You can add dev dependencies with the --dev switch.","title":"The workflow"},{"location":"dev/#talking-of-dev-dependencies","text":"As well as the test dependencies I find the following useful to have in my dev environment. Black for code formatting Pydocstyle to check my docstrings mypy to check my type hints (Pycharm does this automatically)","title":"Talking of dev dependencies..."},{"location":"docs/","text":"I've found pdocs to be a pretty neat way to quickly produce API documentation for a project. I simply add pdocs as a dev dependency in poetry and run poetry run pdocs","title":"Docs"},{"location":"install/","text":"Installing on the target system Overview When distributing my application to the target system, I provide a batch (Windows) and bash (Linux) scripts along with the wheel created by Poetry. I'll also provide a readme and any configuration or resource files required. The script does the following: Creates a directory in a location of the user's choosing Creates a virtual environment in an env subdirectory Activates the virtual environment and pip installs my project from the wheel Copies readme and resources into the target directory Makes a link to the script we declared in the pyproject.toml files (which by defaul is created several folders deep inside the virtual enviroment) at the top level of the new directory This leaves you with something like this, which looks remarkably like a 'normal' Windows application. You can double click the executable to run the app, or make it the target of a service and it will just work. Importantly for me, it works identically on Linux. my-app/ \u251c\u2500 env/ \u251c\u2500 resources/ \u251c\u2500 my-app.exe \u251c\u2500 readme.md \u251c\u2500 setup.yaml I've been considering creating a .deb package, or as a minimum installing into /usr/local/bin on Debian to fit in with the norms of Linux, but I haven't got that far How to create an executable You can tell pip (or equivalent) to create executables by declaring scripts in pyproject.toml : [tool.poetry.scripts] my-executable = \"my_package.my_module:my_function\" The installation script This is the Linux version, both versions are on my github. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/usr/bin/env bash set -e # (1) echo \"*************************\" echo \"Installing My Great Project\" echo \"*************************\" filePath = ${ 1 :- \"/opt\" } /my-great-project # (2) echo \"Installing to $filePath \" echo \"Building Python virtual environment\" python3 -m venv $filePath /env # (3) target =( my_great_project*.whl ) # (4) source $filePath /env/bin/activate # (5) pip install \" ${ target [0] } \" # (6) ln -s $filePath /env/bin/my-great-project $filePath /my-great-project # (7) cp -R assets $filePath /assets # (8) cp setup.yaml $filePath /setup.yaml stop if there's an error allow user to specify install location or default to /opt create virtual environment inside the installation folder use wildcard to allow for version number activate the venv we just created install your project from the wheel file create a symlink to the script for convenience copy any other files that you need to distribute with the app","title":"Installing on the target system"},{"location":"install/#installing-on-the-target-system","text":"","title":"Installing on the target system"},{"location":"install/#overview","text":"When distributing my application to the target system, I provide a batch (Windows) and bash (Linux) scripts along with the wheel created by Poetry. I'll also provide a readme and any configuration or resource files required. The script does the following: Creates a directory in a location of the user's choosing Creates a virtual environment in an env subdirectory Activates the virtual environment and pip installs my project from the wheel Copies readme and resources into the target directory Makes a link to the script we declared in the pyproject.toml files (which by defaul is created several folders deep inside the virtual enviroment) at the top level of the new directory This leaves you with something like this, which looks remarkably like a 'normal' Windows application. You can double click the executable to run the app, or make it the target of a service and it will just work. Importantly for me, it works identically on Linux. my-app/ \u251c\u2500 env/ \u251c\u2500 resources/ \u251c\u2500 my-app.exe \u251c\u2500 readme.md \u251c\u2500 setup.yaml I've been considering creating a .deb package, or as a minimum installing into /usr/local/bin on Debian to fit in with the norms of Linux, but I haven't got that far","title":"Overview"},{"location":"install/#how-to-create-an-executable","text":"You can tell pip (or equivalent) to create executables by declaring scripts in pyproject.toml : [tool.poetry.scripts] my-executable = \"my_package.my_module:my_function\"","title":"How to create an executable"},{"location":"install/#the-installation-script","text":"This is the Linux version, both versions are on my github. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/usr/bin/env bash set -e # (1) echo \"*************************\" echo \"Installing My Great Project\" echo \"*************************\" filePath = ${ 1 :- \"/opt\" } /my-great-project # (2) echo \"Installing to $filePath \" echo \"Building Python virtual environment\" python3 -m venv $filePath /env # (3) target =( my_great_project*.whl ) # (4) source $filePath /env/bin/activate # (5) pip install \" ${ target [0] } \" # (6) ln -s $filePath /env/bin/my-great-project $filePath /my-great-project # (7) cp -R assets $filePath /assets # (8) cp setup.yaml $filePath /setup.yaml stop if there's an error allow user to specify install location or default to /opt create virtual environment inside the installation folder use wildcard to allow for version number activate the venv we just created install your project from the wheel file create a symlink to the script for convenience copy any other files that you need to distribute with the app","title":"The installation script"},{"location":"jenkins/","text":"Automate packaging with Jenkins Jenkins is a handy tool to automate all the packaging steps I've documented on this site. It's especially useful if you are sharing your applications with a wider team who have access to Jenkins. In this scenario, all you need to do is to commit your code and then anyone who wants a copy of the tested, deployable application can grab it from Jenkins. You can have Jenkins build every time you commit, nightly, or just build on-demand. Getting Jenkins to perform your testing and packaging is pretty straightforward. You need to create a Jenkins job, associate your Git repository with it and then add shell script to perform all the steps documented on this site. It should look something like this. Shell script to be executed by Jenkins 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Make all required modifications to PATH export PATH = \" $HOME /.local/bin: $PATH \" export PYENV_ROOT = \" $HOME /.pyenv\" export PATH = \" $PYENV_ROOT /bin: $PATH \" eval \" $( pyenv init --path ) \" eval \" $( pyenv init - ) \" eval \" $( pyenv virtualenv-init - ) \" # Make all required versions of Python available for test pyenv local 3 .8.12 3 .9.7 3 .10.0 # Run tests and build chmod +x build_current_version.sh ./build_current_version.sh # Build the API docs poetry install poetry run python -m pdoc example_package -d numpy -o ./pdoc # Zip up the dist folder for distribution ( cd dist && zip -r ../example-package- \" $( cat ../example_project/version.txt ) \" .zip ./* ) ( cd pdoc && zip -r ../api-docs-for-developers- \" $( cat ../example_project/version.txt ) \" .zip ./* ) The script build_current_version.sh is separated purely for convenience so I can easily run it in my dev environment. build_current_version.sh 1 2 3 4 5 #!/usr/bin/env bash tox || { echo \"TESTS FAILED - BUILD ABORTED\" ; exit 1 ; } echo \"Test passed - building sdist and wheel\" poetry build || { echo \"POETRY BUILD FAILED - BUILD ABORTED\" ; exit 1 ; }","title":"Automate packaging with Jenkins"},{"location":"jenkins/#automate-packaging-with-jenkins","text":"Jenkins is a handy tool to automate all the packaging steps I've documented on this site. It's especially useful if you are sharing your applications with a wider team who have access to Jenkins. In this scenario, all you need to do is to commit your code and then anyone who wants a copy of the tested, deployable application can grab it from Jenkins. You can have Jenkins build every time you commit, nightly, or just build on-demand. Getting Jenkins to perform your testing and packaging is pretty straightforward. You need to create a Jenkins job, associate your Git repository with it and then add shell script to perform all the steps documented on this site. It should look something like this. Shell script to be executed by Jenkins 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Make all required modifications to PATH export PATH = \" $HOME /.local/bin: $PATH \" export PYENV_ROOT = \" $HOME /.pyenv\" export PATH = \" $PYENV_ROOT /bin: $PATH \" eval \" $( pyenv init --path ) \" eval \" $( pyenv init - ) \" eval \" $( pyenv virtualenv-init - ) \" # Make all required versions of Python available for test pyenv local 3 .8.12 3 .9.7 3 .10.0 # Run tests and build chmod +x build_current_version.sh ./build_current_version.sh # Build the API docs poetry install poetry run python -m pdoc example_package -d numpy -o ./pdoc # Zip up the dist folder for distribution ( cd dist && zip -r ../example-package- \" $( cat ../example_project/version.txt ) \" .zip ./* ) ( cd pdoc && zip -r ../api-docs-for-developers- \" $( cat ../example_project/version.txt ) \" .zip ./* ) The script build_current_version.sh is separated purely for convenience so I can easily run it in my dev environment. build_current_version.sh 1 2 3 4 5 #!/usr/bin/env bash tox || { echo \"TESTS FAILED - BUILD ABORTED\" ; exit 1 ; } echo \"Test passed - building sdist and wheel\" poetry build || { echo \"POETRY BUILD FAILED - BUILD ABORTED\" ; exit 1 ; }","title":"Automate packaging with Jenkins"},{"location":"package/","text":"Packaging the project This part of things had me totally confused for a while. But it turns out recent improvements have made this somewhat simpler. Basically I wanted a way to allow a user to recreate my application and all its dependencies inside a virtual environment with zero Python knowledge and minimal complexity (for me and them). Packaging in Python is such a perenial topic of confusion and debate that there is actually an official organisation dedicated to it. The PYPA site is a useful resource but I found it wasn't particularly friendly to people just looking for a simple opinionated solution. Poetry I use Poetry to manage dependencies and provide a virtual environment when developing. The great thing about Poetry is that packaging essentially comes for free. All I need to do is type poetry build and my project is packaged up into a tarball and a wheel. These contain my code and the metadata defining its dependencies. The metadata also records that Poetry is the tool required to build the package from source (i.e. the tarball, also known as an sdist). Doesn't that mean the user needs Poetry installed? No, thankfully not. That's one of the key reasons I chose this approach. Since PEP-517 and 518, pip looks in the pyproject.toml file to determine what build tools are required when installing a package from sdist, and retrieves these automatically. Alternatively, if you install from the wheel file... Why bother packaging at all, why not just distibute the code? First of all, in the case of pure Python packages your source code is distributed. Once someone has installed you package they can happily import it into their own projects or just browse your .py files. For the general Python community, the biggest advantage of packaging your code is that it means you can upload it to pypi, allowing others to discover and install it. Packaging is also an essential step if your code contains C extensions. However, I'm talking about distributing pur Python applications to users directly, so why bother? The answers: Scripts Version control Scripts are executable files that run some part of your code. See here for more info, but suffice to say the ability to create an executable if very useful if you want to offer a simple 'double-click here to run' user experience. By version control I mean keeping control of the versions of your code that are out in the wild. That can be hard to do if you just commit your code to a repo and allow people to pull it and run it at any moment. For a start you have to be sure that every commit works and is uniquely identifiable by the end-user. Sure, there are ways to do this, but packaging provides a neat way of saying \"at the moment I press build this, and only this, is version 1.2.3 and there will never be another version 1.2.3\". Having a distinct 'build/package' step in your workflow separates what you ship to users from what you commit as source. If you are responsible, even informally for supporting yoru code once it goes out into the ide world then reducing the number of variations you have to support is critical to maintaining sanity. Other things I looked at I initially had a very similar workflow but using pipenv instead of Poetry. It worked alright for sharing code with users, but was missing all the packaging and testing goodness. I also like Pyinstaller as it means the target system doesn't need Python or an internet connection. I might add a Pyinstaller workflow to this site one day.","title":"Packaging the project"},{"location":"package/#packaging-the-project","text":"This part of things had me totally confused for a while. But it turns out recent improvements have made this somewhat simpler. Basically I wanted a way to allow a user to recreate my application and all its dependencies inside a virtual environment with zero Python knowledge and minimal complexity (for me and them). Packaging in Python is such a perenial topic of confusion and debate that there is actually an official organisation dedicated to it. The PYPA site is a useful resource but I found it wasn't particularly friendly to people just looking for a simple opinionated solution.","title":"Packaging the project"},{"location":"package/#poetry","text":"I use Poetry to manage dependencies and provide a virtual environment when developing. The great thing about Poetry is that packaging essentially comes for free. All I need to do is type poetry build and my project is packaged up into a tarball and a wheel. These contain my code and the metadata defining its dependencies. The metadata also records that Poetry is the tool required to build the package from source (i.e. the tarball, also known as an sdist).","title":"Poetry"},{"location":"package/#doesnt-that-mean-the-user-needs-poetry-installed","text":"No, thankfully not. That's one of the key reasons I chose this approach. Since PEP-517 and 518, pip looks in the pyproject.toml file to determine what build tools are required when installing a package from sdist, and retrieves these automatically. Alternatively, if you install from the wheel file...","title":"Doesn't that mean the user needs Poetry installed?"},{"location":"package/#why-bother-packaging-at-all-why-not-just-distibute-the-code","text":"First of all, in the case of pure Python packages your source code is distributed. Once someone has installed you package they can happily import it into their own projects or just browse your .py files. For the general Python community, the biggest advantage of packaging your code is that it means you can upload it to pypi, allowing others to discover and install it. Packaging is also an essential step if your code contains C extensions. However, I'm talking about distributing pur Python applications to users directly, so why bother? The answers: Scripts Version control Scripts are executable files that run some part of your code. See here for more info, but suffice to say the ability to create an executable if very useful if you want to offer a simple 'double-click here to run' user experience. By version control I mean keeping control of the versions of your code that are out in the wild. That can be hard to do if you just commit your code to a repo and allow people to pull it and run it at any moment. For a start you have to be sure that every commit works and is uniquely identifiable by the end-user. Sure, there are ways to do this, but packaging provides a neat way of saying \"at the moment I press build this, and only this, is version 1.2.3 and there will never be another version 1.2.3\". Having a distinct 'build/package' step in your workflow separates what you ship to users from what you commit as source. If you are responsible, even informally for supporting yoru code once it goes out into the ide world then reducing the number of variations you have to support is critical to maintaining sanity.","title":"Why bother packaging at all, why not just distibute the code?"},{"location":"package/#other-things-i-looked-at","text":"I initially had a very similar workflow but using pipenv instead of Poetry. It worked alright for sharing code with users, but was missing all the packaging and testing goodness. I also like Pyinstaller as it means the target system doesn't need Python or an internet connection. I might add a Pyinstaller workflow to this site one day.","title":"Other things I looked at"},{"location":"run-tests/","text":"Run tests with tox Why bother? Running the unit tests we wrote previously is as simple as just executing pytest in the dev environment, so why do anything else? Turns out there are many good reasons: You are not testing your code in a clean environment - there could be extra files, missing ones, environment variables, etc. You are testing the code as it appears in your repository folders, not as it appears after installation with pip You are not testing whether the code can actually be pip installed You are only testing the version of Python you have on your dev system How does tox help? Tox is a really cool bit of software, it does so much with almost no effort at all. With a minimal amount of configuration (see below) you can have tox create multiple virtual environments with different versions of Python, install your package into them and run all your tests. [tox] envlist = py37, py38, py39, py310 isolated_build = true [testenv] deps = pytest pytest-cov pytest-mock commands = pytest --cov = my_project --cov-append --cov-branch Installing tox Since tox will be installing your project, it doesn't make much sense to make tox a project dependency. Instead you should make it available in your test environment independently. The simplest way is just to pip install tox into your system Python. If you'd rather not clutter your system python with packages Managing test dependencies When your project is installed by tox, the dev dependencies are not installed. This is as-expected as this is supposed to replicate a production environment. However, this means that if you want pytest in your environment (hint: you do) you need to ensure you include it as a dependency in the tox.ini file as shown above. Managing multiple Python versions with pyenv One thing that tox doesn't do is install Python for you. You need to have the Python versions with which you want to test installed on your test system. As XKCD confirms , multiple Python installs can quickly get out of hand. Luckily pyenv is here to help. If you are running Jenkins on Windows, there is a Windows-friendly fork pyenv-win","title":"Run tests with tox"},{"location":"run-tests/#run-tests-with-tox","text":"","title":"Run tests with tox"},{"location":"run-tests/#why-bother","text":"Running the unit tests we wrote previously is as simple as just executing pytest in the dev environment, so why do anything else? Turns out there are many good reasons: You are not testing your code in a clean environment - there could be extra files, missing ones, environment variables, etc. You are testing the code as it appears in your repository folders, not as it appears after installation with pip You are not testing whether the code can actually be pip installed You are only testing the version of Python you have on your dev system","title":"Why bother?"},{"location":"run-tests/#how-does-tox-help","text":"Tox is a really cool bit of software, it does so much with almost no effort at all. With a minimal amount of configuration (see below) you can have tox create multiple virtual environments with different versions of Python, install your package into them and run all your tests. [tox] envlist = py37, py38, py39, py310 isolated_build = true [testenv] deps = pytest pytest-cov pytest-mock commands = pytest --cov = my_project --cov-append --cov-branch","title":"How does tox help?"},{"location":"run-tests/#installing-tox","text":"Since tox will be installing your project, it doesn't make much sense to make tox a project dependency. Instead you should make it available in your test environment independently. The simplest way is just to pip install tox into your system Python. If you'd rather not clutter your system python with packages","title":"Installing tox"},{"location":"run-tests/#managing-test-dependencies","text":"When your project is installed by tox, the dev dependencies are not installed. This is as-expected as this is supposed to replicate a production environment. However, this means that if you want pytest in your environment (hint: you do) you need to ensure you include it as a dependency in the tox.ini file as shown above.","title":"Managing test dependencies"},{"location":"run-tests/#managing-multiple-python-versions-with-pyenv","text":"One thing that tox doesn't do is install Python for you. You need to have the Python versions with which you want to test installed on your test system. As XKCD confirms , multiple Python installs can quickly get out of hand. Luckily pyenv is here to help. If you are running Jenkins on Windows, there is a Windows-friendly fork pyenv-win","title":"Managing multiple Python versions with pyenv"},{"location":"tldr/","text":"Quick reference My workflow This is what my workflow looks like: Write software using Poetry to manage dependencies Write tests using Pytest Package the project to tarball and/or wheel using Poetry Use Tox to automatically run the tests in isolated environment(s) Distribute the package with an installer script Double click the executable created by the install script to run the application! Optional extras There are some optional steps that can be useful: Single-source and automatically increment the version number Automatically generate documentation using pdoc Perform all testing and packaging using Jenkins","title":"Quick reference"},{"location":"tldr/#quick-reference","text":"","title":"Quick reference"},{"location":"tldr/#my-workflow","text":"This is what my workflow looks like: Write software using Poetry to manage dependencies Write tests using Pytest Package the project to tarball and/or wheel using Poetry Use Tox to automatically run the tests in isolated environment(s) Distribute the package with an installer script Double click the executable created by the install script to run the application!","title":"My workflow"},{"location":"tldr/#optional-extras","text":"There are some optional steps that can be useful: Single-source and automatically increment the version number Automatically generate documentation using pdoc Perform all testing and packaging using Jenkins","title":"Optional extras"},{"location":"versions/","text":"Version numbering Single-sourcing the version number Having a single, definitive version number for your project is obviously highly desirable - why bother with version numbers at all if they are ambiguous? So it's very handy that the pyproject.toml file contains a version attribute. Poetry also provides the poetry version command to access or increment the version number. If you're looking for maximum simplicity, you can stop there , but as we shall see below there are still some traps lurking for the unwary. Accessing the version number Once you have a version number, you are likely to want to use it. You might want to log the version number on startup or print it to your user interface. You may also want to provide a __version__ module attribute, a common convention to allow others to easily check the version of your package they are using. Using the above method you get three out-of-the-box ways to access the version number, but none of them is fool-proof Use poetry version to return it Read it out of the pyproject.toml file Get it from importlib.metadata Option (1) is fine during development or packaging but no use once you've distributed the package as it requires you to have poetry installed and to have installed your project using poetry install . Option (2) you could just about get away with during development, but in deployment it would be a mess. The pyproject.toml file is meant for use when packaging or installing, not during runtime. For a start you'd have to find the file buried in your site-packages directory, then you'd need the non-standard toml package installed to pass it (lets skip over the fact Python is trying to standardise on a file in a format it doesn't actually support in the standard library) Option (3) is the 'correct' one and avoids all the issues above. All you need to do is: 1 2 from importlib.metadata import version __version__ = version ( \"my_package_name\" ) Frustratingingly, this has two drawbacks: It requires Python 3.8 which makes it a non-starter for the Debian 10 servers I often deploy to, which use 3.7. It only works after the code has been packaged and installed. If you try to invoke it when running your code from the dev environment it won't work, which is annoying. What I actually do To overcome the various limitations above, I do the following To increment the version number, I use a script that wraps poetry version and writes a VERSION.TXT file into each package within my project To access the version number, I read the number in from VERSION.TXT in __init__.py and assign the name __version__ to it. My full script also includes an experimental implementation of C# style version numbers, but a stripped down version looks like this.","title":"Version numbering"},{"location":"versions/#version-numbering","text":"","title":"Version numbering"},{"location":"versions/#single-sourcing-the-version-number","text":"Having a single, definitive version number for your project is obviously highly desirable - why bother with version numbers at all if they are ambiguous? So it's very handy that the pyproject.toml file contains a version attribute. Poetry also provides the poetry version command to access or increment the version number. If you're looking for maximum simplicity, you can stop there , but as we shall see below there are still some traps lurking for the unwary.","title":"Single-sourcing the version number"},{"location":"versions/#accessing-the-version-number","text":"Once you have a version number, you are likely to want to use it. You might want to log the version number on startup or print it to your user interface. You may also want to provide a __version__ module attribute, a common convention to allow others to easily check the version of your package they are using. Using the above method you get three out-of-the-box ways to access the version number, but none of them is fool-proof Use poetry version to return it Read it out of the pyproject.toml file Get it from importlib.metadata Option (1) is fine during development or packaging but no use once you've distributed the package as it requires you to have poetry installed and to have installed your project using poetry install . Option (2) you could just about get away with during development, but in deployment it would be a mess. The pyproject.toml file is meant for use when packaging or installing, not during runtime. For a start you'd have to find the file buried in your site-packages directory, then you'd need the non-standard toml package installed to pass it (lets skip over the fact Python is trying to standardise on a file in a format it doesn't actually support in the standard library) Option (3) is the 'correct' one and avoids all the issues above. All you need to do is: 1 2 from importlib.metadata import version __version__ = version ( \"my_package_name\" ) Frustratingingly, this has two drawbacks: It requires Python 3.8 which makes it a non-starter for the Debian 10 servers I often deploy to, which use 3.7. It only works after the code has been packaged and installed. If you try to invoke it when running your code from the dev environment it won't work, which is annoying.","title":"Accessing the version number"},{"location":"versions/#what-i-actually-do","text":"To overcome the various limitations above, I do the following To increment the version number, I use a script that wraps poetry version and writes a VERSION.TXT file into each package within my project To access the version number, I read the number in from VERSION.TXT in __init__.py and assign the name __version__ to it. My full script also includes an experimental implementation of C# style version numbers, but a stripped down version looks like this.","title":"What I actually do"},{"location":"write-tests/","text":"Writing tests Use pytest Unit testing is an area I'm still learning. I initially based my tests on this excellent article , particularly part 2 . However I found that two of its key recommendations, inheriting from unittest.TestCase and use of the parameterized package, seemed to make things harder when I came to do more complicated tests. For example it was difficult to find any help on how to use unittest.mock.patch (or my current choice, pytest-mock , which wraps this into mocker ) with parameterized. Sticking within the pytest package has made things easier in this regard. Therefore, my current approach to unit tests is to write bare functions (no classes) with plain assert statements. If I need to parameterize a function I use pytest.parametrize and if I need to patch something I use mocker from pytest-mock. For other mocking I use the classic unittest.MagicMock . Where possible I try to write code that avoids the need for patching during tests. I use dependency injection or callbacks such that if function A needs to call function B, function B is passed to function A as an argument. For example: 1 2 3 4 5 6 7 8 9 10 def read_data_from_disk ( selection ): # some code to find the data etc def get_data ( data_getter ): # lots of other stuff selection = input ( \"Select your data: \" ) data = data_getter ( selection ) # lots more stuff get_data ( read_data_from_disk ) Test dependencies To let me quickly run my tests while developing I like to install my test dependencies into my dev environment using poetry add --dev . This adds them to pyproject.toml show shown. ini linenums=1 [tool.poetry.dev-dependencies] pytest = \"^6.2.5\" pytest-cov = \"^2.12.1\" pytest-mock = \"^3.6.1\"","title":"Writing tests"},{"location":"write-tests/#writing-tests","text":"","title":"Writing tests"},{"location":"write-tests/#use-pytest","text":"Unit testing is an area I'm still learning. I initially based my tests on this excellent article , particularly part 2 . However I found that two of its key recommendations, inheriting from unittest.TestCase and use of the parameterized package, seemed to make things harder when I came to do more complicated tests. For example it was difficult to find any help on how to use unittest.mock.patch (or my current choice, pytest-mock , which wraps this into mocker ) with parameterized. Sticking within the pytest package has made things easier in this regard. Therefore, my current approach to unit tests is to write bare functions (no classes) with plain assert statements. If I need to parameterize a function I use pytest.parametrize and if I need to patch something I use mocker from pytest-mock. For other mocking I use the classic unittest.MagicMock . Where possible I try to write code that avoids the need for patching during tests. I use dependency injection or callbacks such that if function A needs to call function B, function B is passed to function A as an argument. For example: 1 2 3 4 5 6 7 8 9 10 def read_data_from_disk ( selection ): # some code to find the data etc def get_data ( data_getter ): # lots of other stuff selection = input ( \"Select your data: \" ) data = data_getter ( selection ) # lots more stuff get_data ( read_data_from_disk )","title":"Use pytest"},{"location":"write-tests/#test-dependencies","text":"To let me quickly run my tests while developing I like to install my test dependencies into my dev environment using poetry add --dev . This adds them to pyproject.toml show shown. ini linenums=1 [tool.poetry.dev-dependencies] pytest = \"^6.2.5\" pytest-cov = \"^2.12.1\" pytest-mock = \"^3.6.1\"","title":"Test dependencies"}]}