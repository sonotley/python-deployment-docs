{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This site is intended as a narrative on how and why I devised the snoap workflow.  It's not a complete definition of the workflow or documentation for it. If that's what you're looking for check out cookiecutter-snoap  which is kind of reference implementation for snoap and probably gets updated more than this site does.</p>"},{"location":"#what-is-this-all-about","title":"What is this all about?","text":"<p>Python is pretty easy to write, but when it comes to packaging, distributing and running code somewhere other than your own computer, things get a bit tricky. This is a guide to how I've chosen to tackle that problem. It's a reference for my future self and hopefully of help to other people too.</p>"},{"location":"#who-is-this-for","title":"Who is this for?","text":"<p>In general, I have two target use-cases when I'm writing code, if you have similar needs perhaps you'll find this site useful.</p> <ul> <li>I'm providing a utility for users to use on their workstations</li> <li>I'm proving software to be run as a service on a server</li> </ul> <p>When I started this project, these workstations and servers were often Windows, and that has shaped my approach to some extent.  For example, I have a preference for keeping everything in a single directory windows-style.</p> <p>The two use-cases share some similarities:</p> <ul> <li>I can't guarantee the version of Python (or even the existence of Python) on the target system</li> <li>I don't want the user to need to know any Python or to do lots of setup work, it needs to just install and work</li> <li>I don't want the deployed code to be scattered around non-obvious places, ideally everything should be in one directory so it can be uninstalled simply by deleting that directory</li> </ul> <p>In the case of deployment to a server there is an important addition to the third point above:</p> <ul> <li>The installed software must not be dependent on the continued existence of the user account by which it was installed.</li> </ul>"},{"location":"#who-is-this-not-for","title":"Who is this not for?","text":"<p>Well, I like to think anyone with an interest in Python could get something from these pages, but there are a few area I'm not planning on addressing simply because they aren't options I generally use. They aren't bad options, they just aren't part of the environment I'm currently working in. They are:</p> <ul> <li>Deploying software as a Docker container</li> <li>Deploying to 'serverless' cloud services such as Google App Engine, AWS Lambda, Heroku</li> </ul>"},{"location":"dev/","title":"Development environment","text":""},{"location":"dev/#the-basics","title":"The basics","text":"<p>My development environment consists of:</p> <ul> <li>Pycharm with Poetry plugin</li> <li>Poetry</li> <li>Git</li> </ul> <p>Pycharm is largely just a personal preference, you can follow this workflow with any IDE, or none. Similarly Git is central to how I share, backup and manage the history of my source code, but has no place in my deployment workflow other than as a way to get the source to the place it will be packaged. Poetry however, is pretty important.</p>"},{"location":"dev/#what-is-poetry-ive-never-heard-of-it","title":"What is Poetry? I've never heard of it.","text":"<p>Poetry is a fairly young project which provides an all-in-one solution to packaging and dependency management. I use Poetry because it makes it very easy to package my code up into something that someone else can <code>pip install</code> (or indeed <code>poetry install</code> but as you'll see, I don't want to rely on the target system having Poetry installed).</p>"},{"location":"dev/#the-workflow","title":"The workflow","text":"<p>When starting a new project, either create it using <code>poetry new</code> or select Poetry from the available interpreters in Pycharm. This will create a basic project structure including two packages, one for your source and one for your tests. It will also create a virtual environment (not in the project directory by default) and <code>pyproject.toml</code> file.</p> <p>The <code>pyproject.toml</code> seems to be the source of some mild controversy in the Python community, but we can ignore that. The great thing for us is that it's the only configuration file we need for our project. </p> <p>To install additional packages from PyPI, rather than using <code>pip</code>, just use <code>poetry add</code>. This will install the package into your environment and add it to the dependencies section of <code>pyproject.toml</code>. You can add dev dependencies with the <code>--dev</code> switch.</p>"},{"location":"dev/#talking-of-dev-dependencies","title":"Talking of dev dependencies...","text":"<p>As well as the test dependencies I find the following useful to have in my dev environment.</p> <ul> <li>Black for code formatting</li> <li>Pydocstyle to check my docstrings</li> <li>mypy to check my type hints (Pycharm does this automatically)</li> </ul>"},{"location":"docs/","title":"Docs","text":"<p>I've found pdoc to be a pretty neat way to quickly produce API documentation for a project. I simply add pdoc as a dev dependency in poetry and run </p> <pre><code>poetry run pdoc\n</code></pre> <p>That's all it takes to build nice-looking API documentation from my docstrings. This can be bundled with the package or used to build a website.</p>"},{"location":"future/","title":"Things I'd like to try in future","text":""},{"location":"future/#more-idiomatic-linux-installation","title":"More idiomatic Linux installation","text":"<p>My current installation method is really aimed at Windows users. It produces self-contained directories with an executable inside.  However, this executable is not on the path, and you can't install using a package manager - both basic expectations for Linux users.</p> <p>To be honest, I just haven't got my head around creating <code>.deb</code> packages but I am planning to add basic support for placing files in the correct place according to the Filesystem Hierarchy Standard at some point.</p>"},{"location":"future/#using-pdm-instead-of-poetry","title":"Using pdm instead of Poetry","text":"<p>I have been bitten by Poetry's slightly odd approach to pinning maximum versions  (excellent article here) and pdm seems to deal more neatly with this aspect.  As I understand it pdm is intended to be used as a package manager on the target machine, which goes somewhat against my 'no prerequisites' principle.  However, if it were to become ubiquitous like pip or npm, I could certainly see myself using it.</p>"},{"location":"future/#using-hatch-instead-of-poetry","title":"Using Hatch instead of Poetry","text":"<p>Hatch is a really interesting project which I think could take care of generating wheels/sdists in place of Poetry. It also has some cool versioning features which might mean I could retire my script.  Hatch doesn't do dependency resolution, which is fine because <code>pip</code> does that.  However, that does mean you have to manually specify dependencies in <code>pyproject.toml</code> which is a bit tedious and a source of possible errors. I have submitted a feature request to Hatch for this.</p>"},{"location":"future/#bundling-dependencies","title":"Bundling dependencies","text":"<p>I once read a post on the Python forums opining that application deployment should never use the target system's pip; rather the full code including all dependencies should be bundled with the application.  This takes us back into pex territory, so perhaps one day I will revisit pex.</p>"},{"location":"github/","title":"Build and release on GitHub","text":"<p>As I developed cookiecutter-snoap, it was natural for me to adopt GitHub Actions as my platform for automated builds and releases. As such, GitHub actions has completely replaced Jenkins in my workflow.</p> <p>The Github Actions script below is triggered by pushing a tag and performs all the packaging and testing steps. It then publishes the application as a 'release' on the repo.</p> <pre><code>name: Release\n\non:\n  push:\n    tags:\n    - '*'\n\njobs:\n\n  build:\n    runs-on: ubuntu-latest\n    env:\n      PYTHON_KEYRING_BACKEND: keyring.backends.null.Keyring\n    permissions:\n      contents: write\n    steps:\n    - name: setup pyenv\n      uses: \"gabrielfalcao/pyenv-action@v11\"\n      with:\n        default: 3.11\n        versions: 3.8, 3.9, 3.10, 3.11\n    - name: Add poetry\n      uses: abatilo/actions-poetry@v2\n    - uses: actions/checkout@v3\n    - name: Create poetry env\n      run: poetry install --with dev\n    - name: Build project\n      run: ./build/build.sh\n    - name: Zip dist directory\n      uses: vimtor/action-zip@v1\n      with:\n        files: dist/\n        dest: my-lovely-project-${{  github.ref_name }}.zip\n    - uses: ncipollo/release-action@v1\n      with:\n        artifactErrorsFailBuild: true\n        artifacts: my-lovely-project-${{  github.ref_name }}.zip\n        body: \"An automatic build of My lovely project\"\n</code></pre>"},{"location":"install/","title":"Installing on the target system","text":""},{"location":"install/#overview","title":"Overview","text":"<p>When distributing my application to the target system, I provide a batch (Windows) and bash (Linux) scripts along with the wheel created by Poetry. I'll also provide a readme and any configuration or resource files required. The script does the following:</p> <ul> <li>Creates a directory in a location of the user's choosing</li> <li>Creates a virtual environment in an <code>env</code> subdirectory</li> <li>Attempts to <code>pip install</code> the pinned dependencies into that environment from a <code>requirements.txt</code> file</li> <li>Proceeds to <code>pip install</code> my project from the wheel (including dependencies if the previous step failed)</li> <li>Copies readme and resources into the target directory</li> <li>Makes a link to the script we declared in the <code>pyproject.toml</code> files (which by defaul is created several folders deep inside the virtual enviroment) at the top level of the new directory</li> </ul> <p>This leaves you with something like this, which looks remarkably like a 'normal' Windows application. You can double click the executable to run the app, or make it the target of a service and it will just work. Importantly for me, it works identically on Linux.</p> <pre><code>my-app/\n\u251c\u2500 env/\n\u251c\u2500 resources/\n\u251c\u2500 my-app.exe\n\u251c\u2500 readme.md\n\u251c\u2500 setup.yaml\n</code></pre>"},{"location":"install/#how-to-create-an-executable","title":"How to create an executable","text":"<p>You can tell pip (or equivalent) to create executables by declaring scripts in <code>pyproject.toml</code>:</p> <pre><code>[tool.poetry.scripts]\nmy-executable = \"my_package.my_module:my_function\"\n</code></pre>"},{"location":"install/#the-installation-script","title":"The installation script","text":"<p>This is the Linux (specifically <code>bash</code>) version, both versions are included (and more up-to-date) in my cookiecutter.</p> <pre><code>#!/usr/bin/env bash\nset -e\necho \"*************************\"\necho \"Installing My lovely project\"\necho \"*************************\"\nfilePath=${1:-\"/opt\"}/my-lovely-project\necho \"Installing to $filePath\"\necho \"Building Python virtual environment\"\npython3 -m venv \"$filePath\"/env\ntarget=(\"$(dirname \"$0\")\"/my_lovely_project*.whl)\nsource \"$filePath\"/env/bin/activate\npip install -Ur \"$(dirname \"$0\")\"/requirements.txt || { echo Installation with pinned dependencies failed, attempting local dependency resolution; pinFail=1;}\npip install \"${target[0]}\"\nln -s \"$filePath\"/env/bin/my-lovely-project \"$filePath\"/my-lovely-project\ncp -r \"$(dirname \"$0\")\"/config.yaml \"$filePath\"/config.yaml\ncp -r \"$(dirname \"$0\")\"/resources \"$filePath\"/resources\ncp -r \"$(dirname \"$0\")\"/readme.md \"$filePath\"/readme.md\necho \"*******************************\"\necho \"Installation complete\"\nif [ $pinFail == 1 ]\nthen\n  echo WARNING: pinned versions of dependencies could not be installed. Instead dependency resolution was performed by pip, it will probably work but is not exactly as tested.\nfi\necho \"*******************************\"\n</code></pre> <ol> <li>stop if there's an error</li> <li>allow user to specify install location or default to /opt</li> <li>create virtual environment inside the installation folder</li> <li>use wildcard to allow for version number</li> <li>activate the venv we just created</li> <li>install your project from the wheel file</li> <li>create a symlink to the script for convenience</li> <li>copy any other files that you need to distribute with the app</li> </ol>"},{"location":"jenkins/","title":"Automate packaging with Jenkins","text":"<p>When I first created snoap, I was working in a business that had a Jenkins server. I've since moved on to using GitHub Actions, but kept this section of the site for reference.</p>  <p>Jenkins can automate all the packaging steps I've documented on this site. It's especially useful if you are sharing your applications with a wider team who have access to Jenkins. </p> <p>In this scenario, all you need to do is to commit your code and then anyone who wants a copy of the tested, deployable application can grab it from Jenkins. You can have Jenkins build every time you commit, nightly, or just build on-demand.</p> <p>Getting Jenkins to perform your testing and packaging is pretty straightforward. You need to create a Jenkins job, associate your Git repository with it and then add shell script to perform all the steps documented on this site. It should look something like this.</p> Shell script to be executed by Jenkins<pre><code># Make all required modifications to PATH\nexport PATH=\"$HOME/.local/bin:$PATH\"\nexport PYENV_ROOT=\"$HOME/.pyenv\"\nexport PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init --path)\"\neval \"$(pyenv init -)\"\neval \"$(pyenv virtualenv-init -)\"\n# Make all required versions of Python available for test\npyenv local 3.8.12 3.9.7 3.10.0\n# Run tests and build\nchmod +x build/build.sh\n./build/build.sh\n# Build the API docs\npoetry install\npoetry run python -m pdoc example_package -d numpy -o ./pdoc\n# Zip up the dist folder for distribution\n(cd dist &amp;&amp; zip -r ../example-package-\"$(cat ../example_project/version.txt)\".zip ./*)\n(cd pdoc &amp;&amp; zip -r ../api-docs-for-developers-\"$(cat ../example_project/version.txt)\".zip ./*)\n</code></pre> <p>The script <code>build_current_version.sh</code> is separated purely for convenience so I can easily run it in my dev environment.</p> build_current_version.sh<pre><code>#!/usr/bin/env bash\n\ntox || { echo \"TESTS FAILED - BUILD ABORTED\" ; exit 1; }\necho \"Test passed - building sdist and wheel\"\npoetry build || { echo \"POETRY BUILD FAILED - BUILD ABORTED\" ; exit 1; }        \n</code></pre>"},{"location":"others/","title":"Things that I tried but didn't quite work out","text":""},{"location":"others/#pipenv-where-i-started","title":"pipenv- where I started","text":"<p>My earliest version of this workflow involved providing a pipfile and one batch/shell script to install (<code>pipenv install</code>)  and another to run the code inside the virtual environment created by pipenv.  This meant the target system needed to have pipenv installed, which was a minor annoyance.  It also didn't provide an executable or any help with versioning etc.  In any case, pipenv seems to be in decline, although it has had more regular releases again recently.</p>"},{"location":"others/#pipx-applications-with-their-own-environments","title":"pipx - applications with their own environments","text":"<p>During my research I discovered pipx, and I love it.  The approach I describe here can also work with pipx, you simply take the wheel or sdist built with poetry and install it with pipx.  The reason I don't use pipx everywhere is that it creates its virtual environments in an obscure user folder  which means they are only on the path for the user who installed them and could be accidentally deleted if that user's profile is removed (i.e. on a server).  I suspect on Linux this wouldn't be an issue as long as you always <code>sudo pipx install</code> but on Windows it's a show-stopper for server deployment.</p>"},{"location":"others/#pex-portable-python-executables","title":"pex - portable Python executables","text":"<p>These sounded promising - basically a script that brings its own virtual environment without requiring any installation.  But the learning curve was a little too steep and the adoption of pex seems low. I do plan on revisiting pex as a way to provide completely self-contained deployments.</p>"},{"location":"others/#pyinstaller-executables-with-python-included","title":"pyinstaller - executables with Python included","text":"<p>Pyinstaller is great. If you need to package up you application as a single portable executable file it is extremely useful.  Resolving paths can be a little tricky (as it is executed from a temporary directory) but tractable. T he main limitation of Pyinstaller is that it is more like compiled languages - you need to build it on the target platform or you may find your exe doesn't run.  This means having multiple builds and multiple build environments which is all extra management effort.</p>"},{"location":"package/","title":"Packaging the project","text":"<p>The purpose of this step is to allow a user to recreate my application and all its dependencies inside a virtual environment with zero Python knowledge and minimal complexity (for me and them).</p> <p>Packaging in Python is such a perennial topic of confusion and debate that there is actually an official organisation dedicated to it.  The PyPA site is a useful resource, but I found it wasn't particularly friendly to people just looking for a simple opinionated solution.</p>"},{"location":"package/#poetry","title":"Poetry","text":"<p>I use Poetry to manage dependencies and provide a virtual environment when developing. The great thing about Poetry is that packaging essentially comes for free.  All I need to do is type <code>poetry build</code> and my project is packaged up into a tarball and a wheel. These contain my code and the metadata defining its dependencies.  The metadata also records that Poetry is the tool required to build the package from source (i.e. the tarball, also known as an sdist).</p>"},{"location":"package/#doesnt-that-mean-the-user-needs-poetry-installed","title":"Doesn't that mean the user needs Poetry installed?","text":"<p>No, thankfully not. That's one of the key reasons I chose this approach.  Since PEP-517 and 518, pip looks in the <code>pyproject.toml</code> file to determine what build tools are required when installing a package from sdist, and retrieves these automatically.  Better still, Poetry creates a binary wheel as well as an sdist, and wheels can be installed without any 'build backend'.</p>"},{"location":"package/#pinning-dependencies","title":"Pinning dependencies","text":"<p>I'd like to be confident that when a user installs my application, they get an environment as close to the one I tested as possible. To support that, I also use poetry to produce a lock file in the form of a <code>requirements.txt</code> that specifies exactly what versions of every package to use.</p> <pre><code>poetry export -f requirements.txt --output dist/requirements.txt\n</code></pre> <p>This file is used in the test and install steps.</p>"},{"location":"package/#why-bother-packaging-at-all-why-not-just-distribute-the-code","title":"Why bother packaging at all, why not just distribute the code?","text":"<p>For the general Python community, the biggest advantage of packaging your code is that it means you can upload it to [PyPI](https://pypi.org/, allowing others to discover and install it.  Packaging is also an essential step if your code contains C extensions.</p> <p>However, I'm talking about distributing pure Python applications to users directly, so why bother? The answers:</p> <ul> <li>Scripts</li> <li>Version control</li> </ul> <p>Scripts are executable files that run some part of your code. See here for more info, but suffice to say the ability to create an executable if very useful if you want to offer a simple 'double-click here to run' user experience.</p> <p>By version control I mean keeping control of the versions of your code that are out in the wild. That can be hard to do if you just commit your code to a repo and allow people to pull it and run it at any moment.  For a start you have to be sure that every commit works and is uniquely identifiable by the end-user.  Sure, there are ways to do this, but packaging provides a neat way of saying \"at the moment I press 'build' this, and only this, is version 1.2.3 and there will never be another version 1.2.3\". </p> <p>Having a distinct 'build/package' step in your workflow separates what you ship to users from what you commit as source.  If you are responsible, even informally, for supporting your code once it goes out into the wide world then reducing the number of variations you have to support is critical to maintaining sanity.</p>"},{"location":"run-tests/","title":"Run tests with tox","text":""},{"location":"run-tests/#why-bother","title":"Why bother?","text":"<p>Running the unit tests we wrote previously is as simple as just executing <code>pytest</code> in the dev environment, so why do anything else? Turns out there are many good reasons:</p> <ol> <li>You are not testing your code in a clean environment - there could be extra files, missing ones, environment variables, etc.</li> <li>You are testing the code as it appears in your repository folders, not as it appears after installation with <code>pip</code></li> <li>You are not testing whether the code can actually be <code>pip</code> installed</li> <li>You are only testing the version of Python you have on your dev system</li> </ol>"},{"location":"run-tests/#how-does-tox-help","title":"How does tox help?","text":"<p>Tox is a really cool bit of software, it does so much with almost no effort at all. With a minimal amount of configuration (see below)  you can have tox create multiple virtual environments with different versions of Python, install your package into them and run all your tests.</p> <pre><code>    [tox]\n    envlist = py37, py38, py39, py310\n    isolated_build=true\n\n    [testenv]\n    deps =\n        -rdist/requirements.txt\n        pytest\n        pytest-cov\n        pytest-mock\n    commands =\n        pytest --cov=my_project --cov-append --cov-branch\n</code></pre>"},{"location":"run-tests/#installing-tox","title":"Installing tox","text":"<p>Since tox will be installing your project, it doesn't make much sense to make tox a project dependency. Instead you should make it available in your test environment independently. The simplest way is just to <code>pip install tox</code> into your system Python. If you'd rather not clutter your system python with packages</p>"},{"location":"run-tests/#managing-packages-dependencies","title":"Managing packages dependencies","text":"<p>Note the line <code>-rdist/requirements.txt</code> above. This forces tox to <code>pip install</code> all my package's dependencies as declared in <code>requirements.txt</code> before installing the package.  I could skip this step and the dependencies would still get installed as <code>pip</code> would determine they are required for the package.  However, by using a <code>requirements.txt</code> file generated during packaging I can specify the exact versions of every dependency (known as pinning dependencies).  I also include this file with the installer so the production environment looks exactly like the one that was tested.</p>"},{"location":"run-tests/#managing-test-dependencies","title":"Managing test dependencies","text":"<p>When your project is installed by tox, the dev dependencies are not installed. This is as-expected as this is supposed to replicate a production environment. However, this means that if you want pytest in your environment (hint: you do) you need to ensure you include it as a dependency in the tox.ini file as shown above.</p> <p>In the latest version of cookiecutter-snoap I have combined this step with the one above by including the test dependencies in my Poetry environment  and then using the following command to create a combined requirements file, pinning both the runtime and test dependencies.</p> <pre><code>poetry export -f requirements.txt --with dev --output build/requirements-with-dev.txt\n</code></pre>"},{"location":"run-tests/#managing-multiple-python-versions-with-pyenv","title":"Managing multiple Python versions with pyenv","text":"<p>One thing that tox doesn't do is install Python for you. You need to have the Python versions with which you want to test installed on your test system. As XKCD confirms, multiple Python installs can quickly get out of hand. Luckily pyenv is here to help. If you are running on Windows, there is a Windows-friendly fork pyenv-win</p>"},{"location":"tldr/","title":"Quick reference","text":""},{"location":"tldr/#my-workflow","title":"My workflow","text":"<p>This is what my workflow looks like:</p> <ul> <li>Write software using Poetry to manage dependencies</li> <li>Write tests using Pytest</li> <li>Package the project to tarball and/or wheel using Poetry</li> <li>Use Tox to automatically run the tests in isolated environment(s)</li> <li>Distribute the package with an installer script</li> <li>Double-click the executable created by the installation script to run the application!</li> </ul>"},{"location":"tldr/#optional-extras","title":"Optional extras","text":"<p>There are some optional steps that can be useful:</p> <ul> <li>Single-source and automatically increment the version number</li> <li>Automatically generate documentation using pdoc</li> <li>Perform all testing and packaging using GitHub Actions or Jenkins</li> </ul>"},{"location":"tldr/#sounds-good-is-there-a-template-project-that-does-all-this-stuff","title":"Sounds good, is there a template project that does all this stuff?","text":"<p>Yes! Check out cookiecutter-snoap.</p>"},{"location":"tldr/#other-stuff","title":"Other stuff","text":"<p>I've also written up things I tried but didn't ultimately use and things I'd like to try.</p>"},{"location":"versions/","title":"Version numbering","text":""},{"location":"versions/#single-sourcing-the-version-number","title":"Single-sourcing the version number","text":"<p>Having a single, definitive version number for your project is obviously highly desirable - why bother with version numbers at all if they are ambiguous?  So it's very handy that the <code>pyproject.toml</code> file contains a version attribute.  Poetry also provides the <code>poetry version</code> command to access or increment the version number.  If you're looking for maximum simplicity, you can stop there, but as we shall see below there are still some traps lurking for the unwary.</p>"},{"location":"versions/#accessing-the-version-number","title":"Accessing the version number","text":"<p>Once you have a version number, you are likely to want to use it.  You might want to log the version number on startup or print it to your user interface.  You may also want to provide a <code>__version__</code> module attribute, a common convention to allow others to easily check the version of your package they are using.</p> <p>Using the above method you get three out-of-the-box ways to access the version number, but none of them is fool-proof</p> <ol> <li>Use <code>poetry version</code> to return it</li> <li>Read it out of the pyproject.toml file</li> <li>Get it from importlib.metadata</li> </ol> <p>Option (1) is fine during development or packaging but no use once you've distributed the package as it requires you to have poetry installed and to have installed your project using <code>poetry install</code>.</p> <p>Option (2) you could just about get away with during development, but in deployment it would be a mess. The <code>pyproject.toml</code> file is meant for use when packaging or installing, not during runtime.  For a start you'd have to find the file buried in your site-packages directory, then you'd need a toml package installed to parse it (toml support in the standard library is coming in 3.11)</p> <p>Option (3) is the 'correct' one and avoids all the issues above. All you need to do is:</p> <pre><code>from importlib.metadata import version\n__version__ = version(\"my_package_name\")\n</code></pre> <p>Frustratingly, this has two drawbacks:</p> <ul> <li>It requires Python 3.8 which makes it a non-starter for the Debian 10 servers I often deploy to, which use 3.7.</li> <li>It only works after the code has been packaged and installed. If you try to invoke it when running your code from the dev environment it won't work, which is annoying.</li> </ul>"},{"location":"versions/#what-i-actually-do","title":"What I actually do","text":"<p>To overcome the various limitations above, I do the following</p> <ul> <li>To increment the version number, I use a script that wraps poetry version and writes a <code>VERSION.TXT</code> file into each package within my project</li> <li>To access the version number, I read the number in from <code>VERSION.TXT</code> in <code>__init__.py</code> and assign the name <code>__version__</code> to it.</li> </ul> <pre><code>\"\"\"Script to apply versions in projects using Poetry\n\nWraps `poetry version` and creates `version.txt` in each package\n\"\"\"\nimport sys\nimport subprocess\nimport os\n\n\ndef write_version_to_packages(version):\n    dir_list = next(os.walk(os.path.join(os.path.dirname(__file__), \"..\")))[1]\n    for d in dir_list:\n        if d[0] not in (\".\", \"_\") and os.path.exists(os.path.join(d, \"__init__.py\")):\n            target_path = os.path.join(d, \"version.txt\")\n            with open(target_path, 'w') as f:\n                f.write(version)\n\n\nif len(sys.argv) &gt; 2:\n    print(\"Too many arguments\")\n\nelse:\n    subprocess.run([\"poetry\", \"version\", sys.argv[1]])\n    current_version = str(subprocess.check_output([\"poetry\", \"version\"])).strip()\n    write_version_to_packages(current_version)\n</code></pre>"},{"location":"write-tests/","title":"Writing tests","text":""},{"location":"write-tests/#use-pytest","title":"Use pytest","text":"<p>I initially based my tests on this excellent article, particularly part 2. However I found that two of its key recommendations, inheriting from <code>unittest.TestCase</code> and use of the <code>parameterized</code> package, seemed to make things harder when I came to do more complicated tests. For example it was difficult to find any help on how to use <code>unittest.mock.patch</code> (or my current choice, <code>pytest-mock</code>, which wraps this into <code>mocker</code>) with parameterized. Sticking within the pytest package has made things easier in this regard.</p> <p>Therefore, my current approach to unit tests is to write bare functions (no classes) with plain <code>assert</code> statements. If I need to parameterize a function I use <code>pytest.parametrize</code> and if I need to patch something I use <code>mocker</code> from pytest-mock. For other mocking I use the classic <code>unittest.MagicMock</code>.</p> <p>Where possible I try to write code that avoids the need for patching during tests. I use dependency injection or callbacks such that if function A needs to call function B, function B is passed to function A as an argument. For example, the code below makes is easy to have multiple <code>data_getters</code> perhaps one for disk one for database, one for remote API etc. but it also makes it very easy to pass a mock getter for unit testing. Of course I don't do this absolutely everywhere, just in places where is helps.</p> <pre><code>def read_data_from_disk(selection):\n    # some code to find the data etc\n\ndef get_data(data_getter: Callable):\n    # lots of other stuff\n    selection = input(\"Select your data: \")\n    data = data_getter(selection)\n    # lots more stuff\n\nget_data(read_data_from_disk)\n</code></pre>"},{"location":"write-tests/#test-dependencies","title":"Test dependencies","text":"<p>To let me quickly run my tests while developing I like to install my test dependencies into my dev environment using <code>poetry add --dev</code>.  This adds them to <code>pyproject.toml</code> as shown.</p> <p><code>ini linenums=1 [tool.poetry.dev-dependencies] pytest = \"^6.2.5\" pytest-cov = \"^2.12.1\" pytest-mock = \"^3.6.1\"</code></p>"}]}