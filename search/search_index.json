{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This site is currently a work in progress. It's pretty much complete for the original generation of my workflow, but I'm reworking the workflow all the time. Maybe I'll figure out how to add versioning to this site so I can update it non-destructively. What is this all about? Python is pretty easy to write, but when it comes to packaging, distributing and running code somewhere other than your own computer, things get a bit tricky. This is a guide to how I've chosen to tackle that problem. It's a reference for my future self and hopefully of help to other people too. Who is this for? In general, I have two target use-cases when I'm writing code, if you have similar needs perhaps you'll find this site useful. I'm providing a utility for users to use on their workstations - generally Windows or macOS I'm proving software to be run as a service on a server, often Windows but sometimes Linux These two use-cases share some similarities: I can't guarantee the version of Python (or even the existence of Python) on the target system I don't want the installer or user to need to know any Python or to do lots of setup work, it needs to just install and work I don't want the deployed code to be scattered around non-obvious places, ideally everything should be in one directory so it can be uninstalled simply by deleting that directory In the case of deployment to a server there is an important addition to the third point above: The installed software must not be dependent on the continued existence of the user account by which it was installed. Who is this not for? Well, I like to think anyone with an interest in Python could get something from these pages, but there are a few area I'm not planning on addressing simply because they aren't options I generally use. They aren't bad options, they just aren't part of the environment I'm currently working in. They are: Deploying software as a Docker container Deploying to 'serverless' cloud services such as Google App Engine, AWS Lambda, Heroku I also don't really cover the online CI/CD tools such as those provided Github and Gitlab. I'd like to explore these, but they aren't in my current workflow.","title":"Introduction"},{"location":"#introduction","text":"This site is currently a work in progress. It's pretty much complete for the original generation of my workflow, but I'm reworking the workflow all the time. Maybe I'll figure out how to add versioning to this site so I can update it non-destructively.","title":"Introduction"},{"location":"#what-is-this-all-about","text":"Python is pretty easy to write, but when it comes to packaging, distributing and running code somewhere other than your own computer, things get a bit tricky. This is a guide to how I've chosen to tackle that problem. It's a reference for my future self and hopefully of help to other people too.","title":"What is this all about?"},{"location":"#who-is-this-for","text":"In general, I have two target use-cases when I'm writing code, if you have similar needs perhaps you'll find this site useful. I'm providing a utility for users to use on their workstations - generally Windows or macOS I'm proving software to be run as a service on a server, often Windows but sometimes Linux These two use-cases share some similarities: I can't guarantee the version of Python (or even the existence of Python) on the target system I don't want the installer or user to need to know any Python or to do lots of setup work, it needs to just install and work I don't want the deployed code to be scattered around non-obvious places, ideally everything should be in one directory so it can be uninstalled simply by deleting that directory In the case of deployment to a server there is an important addition to the third point above: The installed software must not be dependent on the continued existence of the user account by which it was installed.","title":"Who is this for?"},{"location":"#who-is-this-not-for","text":"Well, I like to think anyone with an interest in Python could get something from these pages, but there are a few area I'm not planning on addressing simply because they aren't options I generally use. They aren't bad options, they just aren't part of the environment I'm currently working in. They are: Deploying software as a Docker container Deploying to 'serverless' cloud services such as Google App Engine, AWS Lambda, Heroku I also don't really cover the online CI/CD tools such as those provided Github and Gitlab. I'd like to explore these, but they aren't in my current workflow.","title":"Who is this not for?"},{"location":"dev/","text":"Development environment The basics My development environment consists of: Pycharm with Poetry plugin Poetry Git Pycharm is largely just a personal preference, you can follow this workflow with any IDE, or none. Similarly Git is central to how I share, backup and manage the history of my source code, but has no place in my deployment workflow other than as a way to get the source to the place it will be packaged. Poetry however, is pretty important. What is Poetry? I've never heard of it. Poetry is a fairly young project which provides an all-in-one solution to packaging and dependency management. I use Poetry because it makes it very easy to package my code up into something that someone else can pip install (or indeed poetry install but as you'll see, I don't want to rely on the target system having Poetry installed). The workflow When starting a new project, either create it using poetry new or select Poetry from the available interpreters in Pycharm. This will create a basic project structure including two packages, one for your source and one for your tests . It will also create a virtual environment (not in the project directory by default) and pyproject.toml file. The pyproject.toml seems to be the source of some mild controversy in the Python community, but we can ignore that. The great thing for us is that it's the only configuration file we need for our project. To install additional packages from PyPI, rather than using pip , just use poetry add . This will install the package into your environment and add it to the dependencies section of pyproject.toml . You can add dev dependencies with the --dev switch. Talking of dev dependencies... As well as the test dependencies I find the following useful to have in my dev environment. Black for code formatting Pydocstyle to check my docstrings mypy to check my type hints (Pycharm does this automatically)","title":"Development environment"},{"location":"dev/#development-environment","text":"","title":"Development environment"},{"location":"dev/#the-basics","text":"My development environment consists of: Pycharm with Poetry plugin Poetry Git Pycharm is largely just a personal preference, you can follow this workflow with any IDE, or none. Similarly Git is central to how I share, backup and manage the history of my source code, but has no place in my deployment workflow other than as a way to get the source to the place it will be packaged. Poetry however, is pretty important.","title":"The basics"},{"location":"dev/#what-is-poetry-ive-never-heard-of-it","text":"Poetry is a fairly young project which provides an all-in-one solution to packaging and dependency management. I use Poetry because it makes it very easy to package my code up into something that someone else can pip install (or indeed poetry install but as you'll see, I don't want to rely on the target system having Poetry installed).","title":"What is Poetry? I've never heard of it."},{"location":"dev/#the-workflow","text":"When starting a new project, either create it using poetry new or select Poetry from the available interpreters in Pycharm. This will create a basic project structure including two packages, one for your source and one for your tests . It will also create a virtual environment (not in the project directory by default) and pyproject.toml file. The pyproject.toml seems to be the source of some mild controversy in the Python community, but we can ignore that. The great thing for us is that it's the only configuration file we need for our project. To install additional packages from PyPI, rather than using pip , just use poetry add . This will install the package into your environment and add it to the dependencies section of pyproject.toml . You can add dev dependencies with the --dev switch.","title":"The workflow"},{"location":"dev/#talking-of-dev-dependencies","text":"As well as the test dependencies I find the following useful to have in my dev environment. Black for code formatting Pydocstyle to check my docstrings mypy to check my type hints (Pycharm does this automatically)","title":"Talking of dev dependencies..."},{"location":"docs/","text":"I've found pdoc to be a pretty neat way to quickly produce API documentation for a project. I simply add pdoc as a dev dependency in poetry and run poetry run pdoc That's all it takes to build nice-looking API documentation from my docstrings. This can be bundled with the package or used to build a website.","title":"Docs"},{"location":"future/","text":"Things I'd like to try in future More idiomatic Linux installation My current installation method is really aimed at Windows users. It produces self-contained directories with an executable inside. However, this executable is not on the path, and you can't install using a package manager - both basic expectations for Linux users. To be honest, I just haven't got my head around creating .deb packages or understanding which combinations of bin , lib , user , usr , .local I should be putting things in. Using pdm instead of Poetry I have heard of pdm and am keen to try it. I have been bitten by Poetry's slightly odd approach to pinning maximum versions (excellent article here ) and pdm seems to deal more neatly with this aspect. As I understand it pdm is intended to be used as a package manager on the target machine, which goes somewhat against my 'no prerequisites' principle. However, if it were to become ubiquitous like pip or npm, I could certainly see myself using it. Using Hatch instead of Poetry Hatch is a really interesting project which I think could take care of generating wheels/sdists in place of Poetry. It also has some cool versioning features which might mean I could retire my script . Hatch doesn't do dependency resolution, which is fine because pip does that. However, that does mean you have to manually specify dependencies in pyproject.toml which is a bit tedious and a source of possible errors. I have submitted a feature request to Hatch for this. Bundling dependencies I once read a post on the Python forums opining that application deployment should never use the target system's pip; rather the full code including all dependencies should be bundled with the application. This takes us back into pex territory, so perhaps one day I will revisit pex.","title":"Things I'd like to try in future"},{"location":"future/#things-id-like-to-try-in-future","text":"","title":"Things I'd like to try in future"},{"location":"future/#more-idiomatic-linux-installation","text":"My current installation method is really aimed at Windows users. It produces self-contained directories with an executable inside. However, this executable is not on the path, and you can't install using a package manager - both basic expectations for Linux users. To be honest, I just haven't got my head around creating .deb packages or understanding which combinations of bin , lib , user , usr , .local I should be putting things in.","title":"More idiomatic Linux installation"},{"location":"future/#using-pdm-instead-of-poetry","text":"I have heard of pdm and am keen to try it. I have been bitten by Poetry's slightly odd approach to pinning maximum versions (excellent article here ) and pdm seems to deal more neatly with this aspect. As I understand it pdm is intended to be used as a package manager on the target machine, which goes somewhat against my 'no prerequisites' principle. However, if it were to become ubiquitous like pip or npm, I could certainly see myself using it.","title":"Using pdm instead of Poetry"},{"location":"future/#using-hatch-instead-of-poetry","text":"Hatch is a really interesting project which I think could take care of generating wheels/sdists in place of Poetry. It also has some cool versioning features which might mean I could retire my script . Hatch doesn't do dependency resolution, which is fine because pip does that. However, that does mean you have to manually specify dependencies in pyproject.toml which is a bit tedious and a source of possible errors. I have submitted a feature request to Hatch for this.","title":"Using Hatch instead of Poetry"},{"location":"future/#bundling-dependencies","text":"I once read a post on the Python forums opining that application deployment should never use the target system's pip; rather the full code including all dependencies should be bundled with the application. This takes us back into pex territory, so perhaps one day I will revisit pex.","title":"Bundling dependencies"},{"location":"install/","text":"Installing on the target system Overview When distributing my application to the target system, I provide a batch (Windows) and bash (Linux) scripts along with the wheel created by Poetry. I'll also provide a readme and any configuration or resource files required. The script does the following: Creates a directory in a location of the user's choosing Creates a virtual environment in an env subdirectory Attempts to pip install the pinned dependencies into that environment from a requirements.txt file Proceeds to pip install my project from the wheel (including dependencies if the previous step failed) Copies readme and resources into the target directory Makes a link to the script we declared in the pyproject.toml files (which by defaul is created several folders deep inside the virtual enviroment) at the top level of the new directory This leaves you with something like this, which looks remarkably like a 'normal' Windows application. You can double click the executable to run the app, or make it the target of a service and it will just work. Importantly for me, it works identically on Linux. my-app/ \u251c\u2500 env/ \u251c\u2500 resources/ \u251c\u2500 my-app.exe \u251c\u2500 readme.md \u251c\u2500 setup.yaml I've been considering creating a .deb package, or as a minimum installing into /usr/local/bin on Debian to fit in with the norms of Linux, but I haven't got that far How to create an executable You can tell pip (or equivalent) to create executables by declaring scripts in pyproject.toml : [tool.poetry.scripts] my-executable = \"my_package.my_module:my_function\" The installation script This is the Linux (specifically bash ) version, both versions are included in my cookiecutter. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/usr/bin/env bash set -e echo \"*************************\" echo \"Installing My lovely project\" echo \"*************************\" filePath = ${ 1 :- \"/opt\" } /my-lovely-project echo \"Installing to $filePath \" echo \"Building Python virtual environment\" python3 -m venv \" $filePath \" /env target =( \" $( dirname \" $0 \" ) \" /my_lovely_project*.whl ) source \" $filePath \" /env/bin/activate pip install -Ur \" $( dirname \" $0 \" ) \" /requirements.txt || { echo Installation with pinned dependencies failed, attempting local dependency resolution ; pinFail = 1 ; } pip install \" ${ target [0] } \" ln -s \" $filePath \" /env/bin/my-lovely-project \" $filePath \" /my-lovely-project cp -r \" $( dirname \" $0 \" ) \" /config.yaml \" $filePath \" /config.yaml cp -r \" $( dirname \" $0 \" ) \" /resources \" $filePath \" /resources cp -r \" $( dirname \" $0 \" ) \" /readme.md \" $filePath \" /readme.md echo \"*******************************\" echo \"Installation complete\" if [ $pinFail == 1 ] then echo WARNING: pinned versions of dependencies could not be installed. Instead dependency resolution was performed by pip, it will probably work but is not exactly as tested. fi echo \"*******************************\" stop if there's an error allow user to specify install location or default to /opt create virtual environment inside the installation folder use wildcard to allow for version number activate the venv we just created install your project from the wheel file create a symlink to the script for convenience copy any other files that you need to distribute with the app","title":"Installing on the target system"},{"location":"install/#installing-on-the-target-system","text":"","title":"Installing on the target system"},{"location":"install/#overview","text":"When distributing my application to the target system, I provide a batch (Windows) and bash (Linux) scripts along with the wheel created by Poetry. I'll also provide a readme and any configuration or resource files required. The script does the following: Creates a directory in a location of the user's choosing Creates a virtual environment in an env subdirectory Attempts to pip install the pinned dependencies into that environment from a requirements.txt file Proceeds to pip install my project from the wheel (including dependencies if the previous step failed) Copies readme and resources into the target directory Makes a link to the script we declared in the pyproject.toml files (which by defaul is created several folders deep inside the virtual enviroment) at the top level of the new directory This leaves you with something like this, which looks remarkably like a 'normal' Windows application. You can double click the executable to run the app, or make it the target of a service and it will just work. Importantly for me, it works identically on Linux. my-app/ \u251c\u2500 env/ \u251c\u2500 resources/ \u251c\u2500 my-app.exe \u251c\u2500 readme.md \u251c\u2500 setup.yaml I've been considering creating a .deb package, or as a minimum installing into /usr/local/bin on Debian to fit in with the norms of Linux, but I haven't got that far","title":"Overview"},{"location":"install/#how-to-create-an-executable","text":"You can tell pip (or equivalent) to create executables by declaring scripts in pyproject.toml : [tool.poetry.scripts] my-executable = \"my_package.my_module:my_function\"","title":"How to create an executable"},{"location":"install/#the-installation-script","text":"This is the Linux (specifically bash ) version, both versions are included in my cookiecutter. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/usr/bin/env bash set -e echo \"*************************\" echo \"Installing My lovely project\" echo \"*************************\" filePath = ${ 1 :- \"/opt\" } /my-lovely-project echo \"Installing to $filePath \" echo \"Building Python virtual environment\" python3 -m venv \" $filePath \" /env target =( \" $( dirname \" $0 \" ) \" /my_lovely_project*.whl ) source \" $filePath \" /env/bin/activate pip install -Ur \" $( dirname \" $0 \" ) \" /requirements.txt || { echo Installation with pinned dependencies failed, attempting local dependency resolution ; pinFail = 1 ; } pip install \" ${ target [0] } \" ln -s \" $filePath \" /env/bin/my-lovely-project \" $filePath \" /my-lovely-project cp -r \" $( dirname \" $0 \" ) \" /config.yaml \" $filePath \" /config.yaml cp -r \" $( dirname \" $0 \" ) \" /resources \" $filePath \" /resources cp -r \" $( dirname \" $0 \" ) \" /readme.md \" $filePath \" /readme.md echo \"*******************************\" echo \"Installation complete\" if [ $pinFail == 1 ] then echo WARNING: pinned versions of dependencies could not be installed. Instead dependency resolution was performed by pip, it will probably work but is not exactly as tested. fi echo \"*******************************\" stop if there's an error allow user to specify install location or default to /opt create virtual environment inside the installation folder use wildcard to allow for version number activate the venv we just created install your project from the wheel file create a symlink to the script for convenience copy any other files that you need to distribute with the app","title":"The installation script"},{"location":"jenkins/","text":"Automate packaging with Jenkins Jenkins is a handy tool to automate all the packaging steps I've documented on this site. It's especially useful if you are sharing your applications with a wider team who have access to Jenkins. In this scenario, all you need to do is to commit your code and then anyone who wants a copy of the tested, deployable application can grab it from Jenkins. You can have Jenkins build every time you commit, nightly, or just build on-demand. Getting Jenkins to perform your testing and packaging is pretty straightforward. You need to create a Jenkins job, associate your Git repository with it and then add shell script to perform all the steps documented on this site. It should look something like this. Shell script to be executed by Jenkins 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Make all required modifications to PATH export PATH = \" $HOME /.local/bin: $PATH \" export PYENV_ROOT = \" $HOME /.pyenv\" export PATH = \" $PYENV_ROOT /bin: $PATH \" eval \" $( pyenv init --path ) \" eval \" $( pyenv init - ) \" eval \" $( pyenv virtualenv-init - ) \" # Make all required versions of Python available for test pyenv local 3 .8.12 3 .9.7 3 .10.0 # Run tests and build chmod +x build_current_version.sh ./build_current_version.sh # Build the API docs poetry install poetry run python -m pdoc example_package -d numpy -o ./pdoc # Zip up the dist folder for distribution ( cd dist && zip -r ../example-package- \" $( cat ../example_project/version.txt ) \" .zip ./* ) ( cd pdoc && zip -r ../api-docs-for-developers- \" $( cat ../example_project/version.txt ) \" .zip ./* ) The script build_current_version.sh is separated purely for convenience so I can easily run it in my dev environment. build_current_version.sh 1 2 3 4 5 #!/usr/bin/env bash tox || { echo \"TESTS FAILED - BUILD ABORTED\" ; exit 1 ; } echo \"Test passed - building sdist and wheel\" poetry build || { echo \"POETRY BUILD FAILED - BUILD ABORTED\" ; exit 1 ; }","title":"Automate packaging with Jenkins"},{"location":"jenkins/#automate-packaging-with-jenkins","text":"Jenkins is a handy tool to automate all the packaging steps I've documented on this site. It's especially useful if you are sharing your applications with a wider team who have access to Jenkins. In this scenario, all you need to do is to commit your code and then anyone who wants a copy of the tested, deployable application can grab it from Jenkins. You can have Jenkins build every time you commit, nightly, or just build on-demand. Getting Jenkins to perform your testing and packaging is pretty straightforward. You need to create a Jenkins job, associate your Git repository with it and then add shell script to perform all the steps documented on this site. It should look something like this. Shell script to be executed by Jenkins 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Make all required modifications to PATH export PATH = \" $HOME /.local/bin: $PATH \" export PYENV_ROOT = \" $HOME /.pyenv\" export PATH = \" $PYENV_ROOT /bin: $PATH \" eval \" $( pyenv init --path ) \" eval \" $( pyenv init - ) \" eval \" $( pyenv virtualenv-init - ) \" # Make all required versions of Python available for test pyenv local 3 .8.12 3 .9.7 3 .10.0 # Run tests and build chmod +x build_current_version.sh ./build_current_version.sh # Build the API docs poetry install poetry run python -m pdoc example_package -d numpy -o ./pdoc # Zip up the dist folder for distribution ( cd dist && zip -r ../example-package- \" $( cat ../example_project/version.txt ) \" .zip ./* ) ( cd pdoc && zip -r ../api-docs-for-developers- \" $( cat ../example_project/version.txt ) \" .zip ./* ) The script build_current_version.sh is separated purely for convenience so I can easily run it in my dev environment. build_current_version.sh 1 2 3 4 5 #!/usr/bin/env bash tox || { echo \"TESTS FAILED - BUILD ABORTED\" ; exit 1 ; } echo \"Test passed - building sdist and wheel\" poetry build || { echo \"POETRY BUILD FAILED - BUILD ABORTED\" ; exit 1 ; }","title":"Automate packaging with Jenkins"},{"location":"others/","text":"Things that I tried but didn't quite work out pipenv - where I started My earliest version of this workflow involved providing a pipfile and one batch/shell script to install ( pipenv install ) and another to run the code inside the virtual environment created by pipenv. This meant the target system needed to have pipenv installed, which was a minor annoyance. It also didn't provide an executable or any help with versioning etc. In any case, pipenv seems to be in decline, although it has had more regular releases again recently. pipx - applications with their own environments During my research I discovered pipx, and I love it. The approach I describe here can also work with pipx, you simply take the wheel or sdist built with poetry and install it with pipx. The reason I don't use pipx everywhere is that it creates its virtual environments in an obscure user folder which means they are only on the path for the user who installed them and could be accidentally deleted if that user's profile is removed (i.e. on a server). I suspect on Linux this wouldn't be an issue as long as you always sudo pipx install but on Windows it's a show-stopper for server deployment. pex - portable Python executables These sounded promising - basically a script that brings its own virtual environment without requiring any installation. But the learning curve was a little too steep and the adoption of pex seems low. I do plan on revisiting pex as a way to provide completely self-contained deployments. pyinstaller - executables with Python included Pyinstaller is great. If you need to package up you application as a single portable executable file it is extremely useful. Resolving paths can be a little tricky (as it is executed from a temporary directory) but tractable. T he main limitation of Pyinstaller is that it is more like compiled languages - you need to build it on the target platform or you may find your exe doesn't run. This means having multiple builds and multiple build environments which is all extra management effort.","title":"Things that I tried but didn't quite work out"},{"location":"others/#things-that-i-tried-but-didnt-quite-work-out","text":"","title":"Things that I tried but didn't quite work out"},{"location":"others/#pipenv-where-i-started","text":"My earliest version of this workflow involved providing a pipfile and one batch/shell script to install ( pipenv install ) and another to run the code inside the virtual environment created by pipenv. This meant the target system needed to have pipenv installed, which was a minor annoyance. It also didn't provide an executable or any help with versioning etc. In any case, pipenv seems to be in decline, although it has had more regular releases again recently.","title":"pipenv- where I started"},{"location":"others/#pipx-applications-with-their-own-environments","text":"During my research I discovered pipx, and I love it. The approach I describe here can also work with pipx, you simply take the wheel or sdist built with poetry and install it with pipx. The reason I don't use pipx everywhere is that it creates its virtual environments in an obscure user folder which means they are only on the path for the user who installed them and could be accidentally deleted if that user's profile is removed (i.e. on a server). I suspect on Linux this wouldn't be an issue as long as you always sudo pipx install but on Windows it's a show-stopper for server deployment.","title":"pipx - applications with their own environments"},{"location":"others/#pex-portable-python-executables","text":"These sounded promising - basically a script that brings its own virtual environment without requiring any installation. But the learning curve was a little too steep and the adoption of pex seems low. I do plan on revisiting pex as a way to provide completely self-contained deployments.","title":"pex - portable Python executables"},{"location":"others/#pyinstaller-executables-with-python-included","text":"Pyinstaller is great. If you need to package up you application as a single portable executable file it is extremely useful. Resolving paths can be a little tricky (as it is executed from a temporary directory) but tractable. T he main limitation of Pyinstaller is that it is more like compiled languages - you need to build it on the target platform or you may find your exe doesn't run. This means having multiple builds and multiple build environments which is all extra management effort.","title":"pyinstaller - executables with Python included"},{"location":"package/","text":"Packaging the project This part of things had me totally confused for a while. But it turns out recent improvements have made this somewhat simpler. Basically I wanted a way to allow a user to recreate my application and all its dependencies inside a virtual environment with zero Python knowledge and minimal complexity (for me and them). Packaging in Python is such a perennial topic of confusion and debate that there is actually an official organisation dedicated to it. The PYPA site is a useful resource, but I found it wasn't particularly friendly to people just looking for a simple opinionated solution. Poetry I use Poetry to manage dependencies and provide a virtual environment when developing. The great thing about Poetry is that packaging essentially comes for free. All I need to do is type poetry build and my project is packaged up into a tarball and a wheel. These contain my code and the metadata defining its dependencies. The metadata also records that Poetry is the tool required to build the package from source (i.e. the tarball, also known as an sdist). A slight extra complexity is dependency pinning. I'd like to be confident that when a user installs my application, they get an environment as close to the one I tested as possible. To support that, I also use poetry to produce a lock file in the form of a requirements.txt : poetry export -f requirements.txt --output dist/requirements.txt Doesn't that mean the user needs Poetry installed? No, thankfully not. That's one of the key reasons I chose this approach. Since PEP-517 and 518, pip looks in the pyproject.toml file to determine what build tools are required when installing a package from sdist, and retrieves these automatically. Better still, Poetry creates a binary wheel as well as an sdist, and wheels can be installed without any 'build backend'. Why bother packaging at all, why not just distibute the code? In the case of pure Python packages your source code is distributed. Once someone has installed you package they can happily import it into their own projects or just browse your .py files. For the general Python community, the biggest advantage of packaging your code is that it means you can upload it to pypi, allowing others to discover and install it. Packaging is also an essential step if your code contains C extensions. However, I'm talking about distributing pur Python applications to users directly, so why bother? The answers: Scripts Version control Scripts are executable files that run some part of your code. See here for more info, but suffice to say the ability to create an executable if very useful if you want to offer a simple 'double-click here to run' user experience. By version control I mean keeping control of the versions of your code that are out in the wild. That can be hard to do if you just commit your code to a repo and allow people to pull it and run it at any moment. For a start you have to be sure that every commit works and is uniquely identifiable by the end-user. Sure, there are ways to do this, but packaging provides a neat way of saying \"at the moment I press build this, and only this, is version 1.2.3 and there will never be another version 1.2.3\". Having a distinct 'build/package' step in your workflow separates what you ship to users from what you commit as source. If you are responsible, even informally for supporting yoru code once it goes out into the ide world then reducing the number of variations you have to support is critical to maintaining sanity. Other things I looked at I initially had a very similar workflow but using pipenv instead of Poetry. It worked alright for sharing code with users, but was missing all the packaging and testing goodness. I also like Pyinstaller as it means the target system doesn't need Python or an internet connection. I might add a Pyinstaller workflow to this site one day.","title":"Packaging the project"},{"location":"package/#packaging-the-project","text":"This part of things had me totally confused for a while. But it turns out recent improvements have made this somewhat simpler. Basically I wanted a way to allow a user to recreate my application and all its dependencies inside a virtual environment with zero Python knowledge and minimal complexity (for me and them). Packaging in Python is such a perennial topic of confusion and debate that there is actually an official organisation dedicated to it. The PYPA site is a useful resource, but I found it wasn't particularly friendly to people just looking for a simple opinionated solution.","title":"Packaging the project"},{"location":"package/#poetry","text":"I use Poetry to manage dependencies and provide a virtual environment when developing. The great thing about Poetry is that packaging essentially comes for free. All I need to do is type poetry build and my project is packaged up into a tarball and a wheel. These contain my code and the metadata defining its dependencies. The metadata also records that Poetry is the tool required to build the package from source (i.e. the tarball, also known as an sdist). A slight extra complexity is dependency pinning. I'd like to be confident that when a user installs my application, they get an environment as close to the one I tested as possible. To support that, I also use poetry to produce a lock file in the form of a requirements.txt : poetry export -f requirements.txt --output dist/requirements.txt","title":"Poetry"},{"location":"package/#doesnt-that-mean-the-user-needs-poetry-installed","text":"No, thankfully not. That's one of the key reasons I chose this approach. Since PEP-517 and 518, pip looks in the pyproject.toml file to determine what build tools are required when installing a package from sdist, and retrieves these automatically. Better still, Poetry creates a binary wheel as well as an sdist, and wheels can be installed without any 'build backend'.","title":"Doesn't that mean the user needs Poetry installed?"},{"location":"package/#why-bother-packaging-at-all-why-not-just-distibute-the-code","text":"In the case of pure Python packages your source code is distributed. Once someone has installed you package they can happily import it into their own projects or just browse your .py files. For the general Python community, the biggest advantage of packaging your code is that it means you can upload it to pypi, allowing others to discover and install it. Packaging is also an essential step if your code contains C extensions. However, I'm talking about distributing pur Python applications to users directly, so why bother? The answers: Scripts Version control Scripts are executable files that run some part of your code. See here for more info, but suffice to say the ability to create an executable if very useful if you want to offer a simple 'double-click here to run' user experience. By version control I mean keeping control of the versions of your code that are out in the wild. That can be hard to do if you just commit your code to a repo and allow people to pull it and run it at any moment. For a start you have to be sure that every commit works and is uniquely identifiable by the end-user. Sure, there are ways to do this, but packaging provides a neat way of saying \"at the moment I press build this, and only this, is version 1.2.3 and there will never be another version 1.2.3\". Having a distinct 'build/package' step in your workflow separates what you ship to users from what you commit as source. If you are responsible, even informally for supporting yoru code once it goes out into the ide world then reducing the number of variations you have to support is critical to maintaining sanity.","title":"Why bother packaging at all, why not just distibute the code?"},{"location":"package/#other-things-i-looked-at","text":"I initially had a very similar workflow but using pipenv instead of Poetry. It worked alright for sharing code with users, but was missing all the packaging and testing goodness. I also like Pyinstaller as it means the target system doesn't need Python or an internet connection. I might add a Pyinstaller workflow to this site one day.","title":"Other things I looked at"},{"location":"run-tests/","text":"Run tests with tox Why bother? Running the unit tests we wrote previously is as simple as just executing pytest in the dev environment, so why do anything else? Turns out there are many good reasons: You are not testing your code in a clean environment - there could be extra files, missing ones, environment variables, etc. You are testing the code as it appears in your repository folders, not as it appears after installation with pip You are not testing whether the code can actually be pip installed You are only testing the version of Python you have on your dev system How does tox help? Tox is a really cool bit of software, it does so much with almost no effort at all. With a minimal amount of configuration (see below) you can have tox create multiple virtual environments with different versions of Python, install your package into them and run all your tests. [tox] envlist = py37, py38, py39, py310 isolated_build = true [testenv] deps = -rdist/requirements.txt pytest pytest-cov pytest-mock commands = pytest --cov = my_project --cov-append --cov-branch Installing tox Since tox will be installing your project, it doesn't make much sense to make tox a project dependency. Instead you should make it available in your test environment independently. The simplest way is just to pip install tox into your system Python. If you'd rather not clutter your system python with packages Managing test dependencies When your project is installed by tox, the dev dependencies are not installed. This is as-expected as this is supposed to replicate a production environment. However, this means that if you want pytest in your environment (hint: you do) you need to ensure you include it as a dependency in the tox.ini file as shown above. Managing multiple Python versions with pyenv One thing that tox doesn't do is install Python for you. You need to have the Python versions with which you want to test installed on your test system. As XKCD confirms , multiple Python installs can quickly get out of hand. Luckily pyenv is here to help. If you are running Jenkins on Windows, there is a Windows-friendly fork pyenv-win","title":"Run tests with tox"},{"location":"run-tests/#run-tests-with-tox","text":"","title":"Run tests with tox"},{"location":"run-tests/#why-bother","text":"Running the unit tests we wrote previously is as simple as just executing pytest in the dev environment, so why do anything else? Turns out there are many good reasons: You are not testing your code in a clean environment - there could be extra files, missing ones, environment variables, etc. You are testing the code as it appears in your repository folders, not as it appears after installation with pip You are not testing whether the code can actually be pip installed You are only testing the version of Python you have on your dev system","title":"Why bother?"},{"location":"run-tests/#how-does-tox-help","text":"Tox is a really cool bit of software, it does so much with almost no effort at all. With a minimal amount of configuration (see below) you can have tox create multiple virtual environments with different versions of Python, install your package into them and run all your tests. [tox] envlist = py37, py38, py39, py310 isolated_build = true [testenv] deps = -rdist/requirements.txt pytest pytest-cov pytest-mock commands = pytest --cov = my_project --cov-append --cov-branch","title":"How does tox help?"},{"location":"run-tests/#installing-tox","text":"Since tox will be installing your project, it doesn't make much sense to make tox a project dependency. Instead you should make it available in your test environment independently. The simplest way is just to pip install tox into your system Python. If you'd rather not clutter your system python with packages","title":"Installing tox"},{"location":"run-tests/#managing-test-dependencies","text":"When your project is installed by tox, the dev dependencies are not installed. This is as-expected as this is supposed to replicate a production environment. However, this means that if you want pytest in your environment (hint: you do) you need to ensure you include it as a dependency in the tox.ini file as shown above.","title":"Managing test dependencies"},{"location":"run-tests/#managing-multiple-python-versions-with-pyenv","text":"One thing that tox doesn't do is install Python for you. You need to have the Python versions with which you want to test installed on your test system. As XKCD confirms , multiple Python installs can quickly get out of hand. Luckily pyenv is here to help. If you are running Jenkins on Windows, there is a Windows-friendly fork pyenv-win","title":"Managing multiple Python versions with pyenv"},{"location":"tldr/","text":"Quick reference My workflow This is what my workflow looks like: Write software using Poetry to manage dependencies Write tests using Pytest Package the project to tarball and/or wheel using Poetry Use Tox to automatically run the tests in isolated environment(s) Distribute the package with an installer script Double-click the executable created by the installation script to run the application! Optional extras There are some optional steps that can be useful: Single-source and automatically increment the version number Automatically generate documentation using pdoc Perform all testing and packaging using Jenkins Other stuff I've also written up things I tried but didn't ultimately use and things I'd like to try .","title":"Quick reference"},{"location":"tldr/#quick-reference","text":"","title":"Quick reference"},{"location":"tldr/#my-workflow","text":"This is what my workflow looks like: Write software using Poetry to manage dependencies Write tests using Pytest Package the project to tarball and/or wheel using Poetry Use Tox to automatically run the tests in isolated environment(s) Distribute the package with an installer script Double-click the executable created by the installation script to run the application!","title":"My workflow"},{"location":"tldr/#optional-extras","text":"There are some optional steps that can be useful: Single-source and automatically increment the version number Automatically generate documentation using pdoc Perform all testing and packaging using Jenkins","title":"Optional extras"},{"location":"tldr/#other-stuff","text":"I've also written up things I tried but didn't ultimately use and things I'd like to try .","title":"Other stuff"},{"location":"versions/","text":"Version numbering Single-sourcing the version number Having a single, definitive version number for your project is obviously highly desirable - why bother with version numbers at all if they are ambiguous? So it's very handy that the pyproject.toml file contains a version attribute. Poetry also provides the poetry version command to access or increment the version number. If you're looking for maximum simplicity, you can stop there , but as we shall see below there are still some traps lurking for the unwary. Accessing the version number Once you have a version number, you are likely to want to use it. You might want to log the version number on startup or print it to your user interface. You may also want to provide a __version__ module attribute, a common convention to allow others to easily check the version of your package they are using. Using the above method you get three out-of-the-box ways to access the version number, but none of them is fool-proof Use poetry version to return it Read it out of the pyproject.toml file Get it from importlib.metadata Option (1) is fine during development or packaging but no use once you've distributed the package as it requires you to have poetry installed and to have installed your project using poetry install . Option (2) you could just about get away with during development, but in deployment it would be a mess. The pyproject.toml file is meant for use when packaging or installing, not during runtime. For a start you'd have to find the file buried in your site-packages directory, then you'd need a toml package installed to parse it (toml support in the standard library is coming in 3.11) Option (3) is the 'correct' one and avoids all the issues above. All you need to do is: 1 2 from importlib.metadata import version __version__ = version ( \"my_package_name\" ) Frustratingly, this has two drawbacks: It requires Python 3.8 which makes it a non-starter for the Debian 10 servers I often deploy to, which use 3.7. It only works after the code has been packaged and installed. If you try to invoke it when running your code from the dev environment it won't work, which is annoying. What I actually do To overcome the various limitations above, I do the following To increment the version number, I use a script that wraps poetry version and writes a VERSION.TXT file into each package within my project To access the version number, I read the number in from VERSION.TXT in __init__.py and assign the name __version__ to it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \"\"\"Script to apply versions in projects using Poetry Wraps `poetry version` and creates `version.txt` in each package \"\"\" import sys import subprocess import os def write_version_to_packages ( version ): dir_list = next ( os . walk ( os . path . join ( os . path . dirname ( __file__ ), \"..\" )))[ 1 ] for d in dir_list : if d [ 0 ] not in ( \".\" , \"_\" ) and os . path . exists ( os . path . join ( d , \"__init__.py\" )): target_path = os . path . join ( d , \"version.txt\" ) with open ( target_path , 'w' ) as f : f . write ( version ) if len ( sys . argv ) > 2 : print ( \"Too many arguments\" ) else : subprocess . run ([ \"poetry\" , \"version\" , sys . argv [ 1 ]]) current_version = str ( subprocess . check_output ([ \"poetry\" , \"version\" ])) . strip () write_version_to_packages ( current_version )","title":"Version numbering"},{"location":"versions/#version-numbering","text":"","title":"Version numbering"},{"location":"versions/#single-sourcing-the-version-number","text":"Having a single, definitive version number for your project is obviously highly desirable - why bother with version numbers at all if they are ambiguous? So it's very handy that the pyproject.toml file contains a version attribute. Poetry also provides the poetry version command to access or increment the version number. If you're looking for maximum simplicity, you can stop there , but as we shall see below there are still some traps lurking for the unwary.","title":"Single-sourcing the version number"},{"location":"versions/#accessing-the-version-number","text":"Once you have a version number, you are likely to want to use it. You might want to log the version number on startup or print it to your user interface. You may also want to provide a __version__ module attribute, a common convention to allow others to easily check the version of your package they are using. Using the above method you get three out-of-the-box ways to access the version number, but none of them is fool-proof Use poetry version to return it Read it out of the pyproject.toml file Get it from importlib.metadata Option (1) is fine during development or packaging but no use once you've distributed the package as it requires you to have poetry installed and to have installed your project using poetry install . Option (2) you could just about get away with during development, but in deployment it would be a mess. The pyproject.toml file is meant for use when packaging or installing, not during runtime. For a start you'd have to find the file buried in your site-packages directory, then you'd need a toml package installed to parse it (toml support in the standard library is coming in 3.11) Option (3) is the 'correct' one and avoids all the issues above. All you need to do is: 1 2 from importlib.metadata import version __version__ = version ( \"my_package_name\" ) Frustratingly, this has two drawbacks: It requires Python 3.8 which makes it a non-starter for the Debian 10 servers I often deploy to, which use 3.7. It only works after the code has been packaged and installed. If you try to invoke it when running your code from the dev environment it won't work, which is annoying.","title":"Accessing the version number"},{"location":"versions/#what-i-actually-do","text":"To overcome the various limitations above, I do the following To increment the version number, I use a script that wraps poetry version and writes a VERSION.TXT file into each package within my project To access the version number, I read the number in from VERSION.TXT in __init__.py and assign the name __version__ to it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \"\"\"Script to apply versions in projects using Poetry Wraps `poetry version` and creates `version.txt` in each package \"\"\" import sys import subprocess import os def write_version_to_packages ( version ): dir_list = next ( os . walk ( os . path . join ( os . path . dirname ( __file__ ), \"..\" )))[ 1 ] for d in dir_list : if d [ 0 ] not in ( \".\" , \"_\" ) and os . path . exists ( os . path . join ( d , \"__init__.py\" )): target_path = os . path . join ( d , \"version.txt\" ) with open ( target_path , 'w' ) as f : f . write ( version ) if len ( sys . argv ) > 2 : print ( \"Too many arguments\" ) else : subprocess . run ([ \"poetry\" , \"version\" , sys . argv [ 1 ]]) current_version = str ( subprocess . check_output ([ \"poetry\" , \"version\" ])) . strip () write_version_to_packages ( current_version )","title":"What I actually do"},{"location":"write-tests/","text":"Writing tests Use pytest I initially based my tests on this excellent article , particularly part 2 . However I found that two of its key recommendations, inheriting from unittest.TestCase and use of the parameterized package, seemed to make things harder when I came to do more complicated tests. For example it was difficult to find any help on how to use unittest.mock.patch (or my current choice, pytest-mock , which wraps this into mocker ) with parameterized. Sticking within the pytest package has made things easier in this regard. Therefore, my current approach to unit tests is to write bare functions (no classes) with plain assert statements. If I need to parameterize a function I use pytest.parametrize and if I need to patch something I use mocker from pytest-mock. For other mocking I use the classic unittest.MagicMock . Where possible I try to write code that avoids the need for patching during tests. I use dependency injection or callbacks such that if function A needs to call function B, function B is passed to function A as an argument. For example, the code below makes is easy to have multiple data_getters perhaps one for disk one for database, one for remote API etc. but it also makes it very easy to pass a mock getter for unit testing. Of course I don't do this absolutely everywhere, just in places where is helps. 1 2 3 4 5 6 7 8 9 10 def read_data_from_disk ( selection ): # some code to find the data etc def get_data ( data_getter : Callable ): # lots of other stuff selection = input ( \"Select your data: \" ) data = data_getter ( selection ) # lots more stuff get_data ( read_data_from_disk ) Test dependencies To let me quickly run my tests while developing I like to install my test dependencies into my dev environment using poetry add --dev . This adds them to pyproject.toml as shown. ini linenums=1 [tool.poetry.dev-dependencies] pytest = \"^6.2.5\" pytest-cov = \"^2.12.1\" pytest-mock = \"^3.6.1\"","title":"Writing tests"},{"location":"write-tests/#writing-tests","text":"","title":"Writing tests"},{"location":"write-tests/#use-pytest","text":"I initially based my tests on this excellent article , particularly part 2 . However I found that two of its key recommendations, inheriting from unittest.TestCase and use of the parameterized package, seemed to make things harder when I came to do more complicated tests. For example it was difficult to find any help on how to use unittest.mock.patch (or my current choice, pytest-mock , which wraps this into mocker ) with parameterized. Sticking within the pytest package has made things easier in this regard. Therefore, my current approach to unit tests is to write bare functions (no classes) with plain assert statements. If I need to parameterize a function I use pytest.parametrize and if I need to patch something I use mocker from pytest-mock. For other mocking I use the classic unittest.MagicMock . Where possible I try to write code that avoids the need for patching during tests. I use dependency injection or callbacks such that if function A needs to call function B, function B is passed to function A as an argument. For example, the code below makes is easy to have multiple data_getters perhaps one for disk one for database, one for remote API etc. but it also makes it very easy to pass a mock getter for unit testing. Of course I don't do this absolutely everywhere, just in places where is helps. 1 2 3 4 5 6 7 8 9 10 def read_data_from_disk ( selection ): # some code to find the data etc def get_data ( data_getter : Callable ): # lots of other stuff selection = input ( \"Select your data: \" ) data = data_getter ( selection ) # lots more stuff get_data ( read_data_from_disk )","title":"Use pytest"},{"location":"write-tests/#test-dependencies","text":"To let me quickly run my tests while developing I like to install my test dependencies into my dev environment using poetry add --dev . This adds them to pyproject.toml as shown. ini linenums=1 [tool.poetry.dev-dependencies] pytest = \"^6.2.5\" pytest-cov = \"^2.12.1\" pytest-mock = \"^3.6.1\"","title":"Test dependencies"}]}